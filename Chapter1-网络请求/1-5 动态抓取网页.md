###  1. 动态加载的实例 

我们在分析要爬取的网页时，先确定目标内容的加载方式。如下图所示，在**拉勾网**搜索爬虫工程师职位，点击下一页，你会发现网址栏的 url 不会变化。

![](imags/20.png)

在 Chrome 浏览器左上角点击**安全**，再点击**禁止** JavaScript，**刷新**网页后发现职位信息全部被隐藏了，由此可见，职位信息是通过 异步 JavaScript 加载的。

![1](imags/5.png)

大多数网页在浏览器中展示的内容都在HTML源代码中。但是，由于主流网站都使用 JavaScript 展现网页内容，和静态网页不同的是，在使用JavaScript时，很多内容并不会出现在HTML源代码中，所以爬取静态网页的技术可能无法正常使用。

#### 1.1 异步加载：

AJAX（Asynchronous Javascript And XML，异步JavaScript和XML）。它的价值在于通过在后台与服务器进行少量数据交换就可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下对网页的某部分进行更新。一方面减少了网页重复内容的下载，另一方面节省了流量，因此AJAX得到了广泛使用。 

传统的传输数据格式方面，使用的是 XML 语法，因此叫 AJAX ,现在在数据交互上更多的是使用 JSON。

#### 1.2 动态抓取网页的方式

- 通过浏览器抓包分析
- 使用 selenium + Chromedriver 模拟浏览器抓取

| 方式           |       优点       | 缺点                                 |
| :------------- | :--------------: | :----------------------------------- |
| 浏览器抓包分析 | 代码量少，效率高 | 分析结构比较费劲，复杂网站加密，难爬 |
| 通过 selenium  |       稳定       | 代码量多，性能低                     |

### 2. 通过浏览器工具抓包分析真实网页地址

第一步： 鼠标右键点击“检查”：

![2](imags/7.png)

第二步： 点击 ‘Network’,然后刷新网页

![3](imags/8.png)

第三步： 点击“XHR”，再点击第一条内容

![4](imags/9.png)

第四步： 点击 Preview → content →positionResult → result

![5](imags/10.png)

通过抓包分析，我们终于找到了要爬取的真实信息！

真实网络地址为：Request URL: 

https://www.lagou.com/jobs/positionAjax.json?city=%E6%9D%AD%E5%B7%9E&needAddtionalResult=false

![](imags/21.png)

点击 Response 复制数据

![](imags/22.png)

打开 https://www.json.cn/ 粘贴数据

![](imags/23.png)

第五步： 点击Headers 查看 request 加载方式

![6](imags/11.png)

我们可以看到 requests 请求方式为 post  ,request请求需要设置 data：

![7](imags/12.png)

```python
data = {'first':true,
		'pn':1,
        'kd:'爬虫工程师'}
# ’first' 判断是否为网页第一页
# 'pn' 网页页码
# 'kd' 搜索的信息项
```

### 3. 通过 selenium + Chromedriver 模拟浏览器抓取

在上面的例子中，通过浏览器抓包分析，找到了真是的 request 请求地址，但是也有一些网站比较复杂，很难找到调用的网页地址，甚至有些网站会进行加密处理。为此我继续学习使用浏览器渲染引擎，直接用浏览器在显示网页时解析HTML、应用 CSS 样式并执行 JavaScript 的语句。

这个方法在爬虫过程中会打开一个浏览器加载该网页，自动操纵浏览器浏览各个网页，顺便把数据抓取下来。即使用浏览器渲染方法将爬取动态网页变成爬取静态网页。

#### 3.1 selenium + Chromedriver 的介绍与安装

##### 3.1.1 什么是selenium ？

- selenium 是一个用于 web 应用程序测试的工具。 selenium 测试直接运行在浏览器中，浏览器自动按照脚本代码做出单击、输入、打开、验证等操作，就像真正的用户在操作一样。
- Chromedriver 是一个 *驱动 Chrome 浏览器* 的驱动程序，使用它才能驱动浏览器。

##### 3.1.2 安装 selenium 4步走：

- 第一步 安装：和其他 python 库一样通过 cmd 中 pip 安装

  ```python 
  pip install selenium
  ```

- 第二步 下载：下载[Chromedriver](http://npm.taobao.org/mirrors/chromedriver/2.33/)，解压后放在环境变量的 path 中，在 Windows 系统中可以将其下载后放在 Anaconda 的安装地址中，如 C:\Users\weiro\Anaconda3\Scripts 为我的安装地址。

- 第三步 设置环境变量：

  - 如图打开系统属性页面

    ![8](imags/14.png)

  - 点击高级系统设置

    ![](imags/15.png)

  - 点击环境变量,选中 path , 点击编辑

    ![](imags/16.png)

  - 在编辑环境变量页面，点击新建，复制 Anaconda 的安装地址,点击确定。如下图，我已经将目录 copy 到 环境变量的 path 中。

    ![](imags/19.png)

- 第四步 使用代码进行测试

  ```python
  from selenium import webdriver       # 导入包
  driver = webdriver.Chrome()          # 打开 Chrome 浏览器
  driver.get('https://www.sogou.com/') # 打开搜狗搜索引擎首页
  print(driver.page_source)            # 获取网页源代码
  ```

![](imags/18.png)

#### 3.2 selenium 常用操作

##### 3.2.1 关闭页面：

- driver.close()  关闭当前页面；

- driver.quit()  退出整个浏览器。

  ```python
  from selenium import webdriver       # 导入包
  import time
  driver = webdriver.Chrome()          # 打开 Chrome 浏览器
  driver.get('https://www.baidu.com/') # 打开百度搜索引擎首页
  time.sleep(10)
  driver.close() # 关闭当前网页
  time.sleep(5)
  driver.quit() # 退出整个浏览器
  ```


##### 3.2.2 定位元素 

以搜狗搜索主页 https://www.sogou.com/ 为例，要使用 selenium 自动打开 Chrome 浏览器 并在输入框中输入 `爬虫`，并搜索。

![](imags/24.png)

- ① find_element_by_id 根据 id 来查找某个元素。

  ```python 
  # 导入包
  from selenium import webdriver
  # 初始化 driver
  driver = webdriver.Chrome()
  url = 'https://www.sogou.com/'
  # 打开 url
  driver.get(url)
  # 根据 id 查找某个元素
  inputTag = driver.find_element_by_id('query')
  # 搜索 “爬虫”
  inputTag.send_keys('爬虫')
  submitTag = driver.find_element(By.ID,'su')
  ```

- ② find_element_by_class_name 根据类名来查找元素

  ```python
  # 导入包
  from seleniume import webdriver 
  # 初始化 driver
  driver = webdriver.Chrome()
  # 打开url
  driver.get(url)
  # 根据类名查找元素
  submitTag = driver.find_element_by_class_name('su')
  ```

- ③ find_element_by_name 根据 name 属性的值来查找元素

  ```python
  # 导入包
  from selenium import webdriver
  # 初始化 driver
  driver = webdriver.Chrome()
  # 打开 url
  driver.get(url)
  # 根据 name 属性的值来查找元素
  driver.find_element_by_name('email')
  ```

- ④ find_element_by_tag_name 根据标签名来查找元素

  ```python
  # 导入包
  from selenium import webdriver
  # 初始化 driver
  driver = webdriver.Chrome()
  # 打开url
  driver.get(url)
  # 根据标签名来查找元素
  driver.find_element_by_tag_name('div')
  ```

- ⑤ find_element_by_xpath 根据 xpath 语法来获取元素

  ```python
  # 导入包
  from selenium import webdriver
  # 初始化 driver
  driver = webdriver.Chrome()
  # 打开 url
  driver.get(url)
  # 根据 xpath 语法来获取元素
  driver.find_element_by_xpath('//div')
  ```

- ⑥ find_element_by_css_selector 根据 css 选择器选择元素

  ```python
  # 导入包
  from selenium import webdriver
  # 初始化 driver
  driver = webdirver.Chrome()
  # 打开 url
  driver.get(url)
  # 根据 CSS 选择器选择元素
  driver.find_element_by_css('//div')
  ```

==注意： find_element 是获取第一个满足条件的元素， find_element**s** 是获取所有满足条件的元素==









